# -*- coding: utf-8 -*-
"""project_credit_card.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uTp6xw9CkYBL_l9jQkPr_0KrPv58qVP0

# **Project Name - Credit Data Fraud Detection**

##Problem Statement:
##Problem Objective:
##Our Goals:
* Understand the little distribution of the "little" data that was provided to us.
* Check the Data ratio of "Fraud" and "Non-Fraud" transactions. 

* Determine the Classifiers we are going to use and decide which one has a higher accuracy.

##Understanding the Data:
####Feature Details:
* PCA Transformation: The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) except for time and amount.

* Scaling: Keep in mind that in order to implement a PCA transformation features need to be previously scaled. (In this case, assuming all  the V features have been scaled )

Importing the required libiraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

from django.conf.locale import nb

new_card1 = pd.read_csv('C:\\Users\\billu\\OneDrive\\Desktop\\creditcard.csv')

X = new_card1.drop(columns='Class', axis=1)
y = new_card1['Class']

from imblearn.over_sampling import SMOTE

oversample = SMOTE()
X, y = oversample.fit_resample(X, y)

ovr = X
ovr['class'] = y

Fea = ovr.iloc[:, 0:30]
x = ovr.drop(columns='class', axis=1)
Y = ovr['class']

for col in Fea.columns:
    q1 = Fea[col].quantile(0.25)
    q3 = Fea[col].quantile(0.75)
    IQR = q3 - q1
    upper_limit = Fea[col][~(Fea[col] > (q3 + 1.5 * IQR))].max()
    lower_limit = Fea[col][~(Fea[col] < (q1 - 1.5 * IQR))].min()
    Fea[col] = np.where(Fea[col] > upper_limit, upper_limit, np.where(Fea[col] < lower_limit, lower_limit, Fea[col]))

Fea.iloc[:, 1:29].boxplot(figsize=(10, 10))
plt.show()

'''#checking data distribution of each feature or column '''

Fea.hist(figsize=(25, 25), ec='w')
plt.show()

"""## Feature Selection:"""

plt.figure(figsize=(20, 12))
correction = Fea.corr()
sns.heatmap(correction, annot=True, cmap=plt.cm.Reds)
plt.show()


def correlation(dt, thres):
    col_corr = set()
    corr_matrix = dt.corr()
    for i in range(len(corr_matrix)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > thres:  # abs takes numbers regardless of sign
                colval = corr_matrix.columns[i]
                col_corr.add(colval)
    return col_corr


corr_var = correlation(Fea, 0.75)
print(corr_var)

Fea = Fea.drop(corr_var, axis=1)  # remove highly correlated independent variables from train data

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(Fea, Y, test_size=0.2, random_state=0)

'''
# Logistic Regression without standardization
from sklearn.linear_model import LogisticRegression

model1 = LogisticRegression()
model1.fit(X_train, y_train)'''



# Logistic Regression
from sklearn.preprocessing import  StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression

model1 = make_pipeline(StandardScaler(), LogisticRegression())  # standardize first then fit model
model1.fit(X_train, y_train)
pickle.dump(model1, open('lg.sav','wb'))
# Accuracy of the model
y_predicted = model1.predict(X_test)
model1.score(X_test, y_test)

model1.score(X_train, y_train)

"""### **MODEL 2 # NAIVE BAYES MODEL**"""

# Naive Bayes model
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB

model2 = make_pipeline(StandardScaler(),
                       GaussianNB())
model2.fit(X_train, y_train)
pickle.dump(model2, open('gNB.sav','wb'))
y_predicted2 = model2.predict(X_test)


"""## **MODEL 3 # RANDOM FOREST**"""

# Random forest 
from sklearn.ensemble import RandomForestClassifier

model3 = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=10))
model3.fit(X_train, y_train)
pickle.dump(model3, open('rfc.sav','wb'))
y_predicted3 = model3.predict(X_test)

